{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8790ec8-ba1f-4ecf-9715-789d4e3e0a47",
   "metadata": {},
   "source": [
    "## Import needed labraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fe4eb-c495-4b20-be20-9314b1412443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015bc704-3b87-437f-ae68-36abc235e9d3",
   "metadata": {},
   "source": [
    "#### Downloading NLTK (Natural Language Toolkit) data\n",
    "#### unzipping it and setting data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460675ac-44fa-4c5f-a04c-62f80512d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet', \"/kaggle/working/nltk_data/\")\n",
    "nltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n",
    "! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n",
    "! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\n",
    "nltk.data.path.append(\"/kaggle/working/nltk_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e03843-303e-4e13-b27e-3484f83f715c",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf9d63-713c-4893-aef5-95c57ca3fdd8",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed615b57-c4ae-4cf6-b06e-73e6b8ee8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data =pd.read_csv(\"twitter_training.csv\")\n",
    "test_data=pd.read_csv(\"twitter_validation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b621e6c3-9c44-4f3d-a118-1401828b90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data.columns = ['Header1', 'company','labels','text']\n",
    "training_data.columns = ['Header1', 'company','labels','text']\n",
    "training_data.drop(columns=[\"Header1\",\"company\"],inplace=True)\n",
    "test_data.drop(columns=[\"Header1\",\"company\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7039c37f-9cc4-4a0f-ac1c-8cda284b7864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:\n",
      "          labels                                               text\n",
      "0      Positive  I am coming to the borders and I will kill you...\n",
      "1      Positive  im getting on borderlands and i will kill you ...\n",
      "2      Positive  im coming on borderlands and i will murder you...\n",
      "3      Positive  im getting on borderlands 2 and i will murder ...\n",
      "4      Positive  im getting into borderlands and i can murder y...\n",
      "...         ...                                                ...\n",
      "74676  Positive  Just realized that the Windows partition of my...\n",
      "74677  Positive  Just realized that my Mac window partition is ...\n",
      "74678  Positive  Just realized the windows partition of my Mac ...\n",
      "74679  Positive  Just realized between the windows partition of...\n",
      "74680  Positive  Just like the windows partition of my Mac is l...\n",
      "\n",
      "[74681 rows x 2 columns]\n",
      "\n",
      "\n",
      "test:\n",
      "          labels                                               text\n",
      "0       Neutral  BBC News - Amazon boss Jeff Bezos rejects clai...\n",
      "1      Negative  @Microsoft Why do I pay for WORD when it funct...\n",
      "2      Negative  CSGO matchmaking is so full of closet hacking,...\n",
      "3       Neutral  Now the President is slapping Americans in the...\n",
      "4      Negative  Hi @EAHelp I’ve had Madeleine McCann in my cel...\n",
      "..          ...                                                ...\n",
      "994  Irrelevant  ⭐️ Toronto is the arts and culture capital of ...\n",
      "995  Irrelevant  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
      "996    Positive  Today sucked so it’s time to drink wine n play...\n",
      "997    Positive  Bought a fraction of Microsoft today. Small wins.\n",
      "998     Neutral  Johnson & Johnson to stop selling talc baby po...\n",
      "\n",
      "[999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"training:\\n\",training_data)\n",
    "print(\"\\n\\ntest:\\n\",test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f153efe-c71a-4b97-8e0c-2dae928d59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment=pd.concat([training_data,test_data],ignore_index=True)\n",
    "sentiment.dropna(inplace=True)\n",
    "sentiment.drop_duplicates(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd78decb-90da-4904-8bca-1bae0231e253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75668</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>♥️ Suikoden 2\\n1️⃣ Alex Kidd in Miracle World\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75669</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Thank you to Matching funds Home Depot RW paym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75671</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Late night stream with the boys! Come watch so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75675</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75676</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70251 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           labels                                               text\n",
       "0        Positive  I am coming to the borders and I will kill you...\n",
       "1        Positive  im getting on borderlands and i will kill you ...\n",
       "2        Positive  im coming on borderlands and i will murder you...\n",
       "3        Positive  im getting on borderlands 2 and i will murder ...\n",
       "4        Positive  im getting into borderlands and i can murder y...\n",
       "...           ...                                                ...\n",
       "75668     Neutral  ♥️ Suikoden 2\\n1️⃣ Alex Kidd in Miracle World\\...\n",
       "75669    Positive  Thank you to Matching funds Home Depot RW paym...\n",
       "75671     Neutral  Late night stream with the boys! Come watch so...\n",
       "75675  Irrelevant  ⭐️ Toronto is the arts and culture capital of ...\n",
       "75676  Irrelevant  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
       "\n",
       "[70251 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a9f6ae-edd5-4b7d-93dd-784e82fbe638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I) # Remove extra white space from text\n",
    "\n",
    "    text = re.sub(r'\\W', ' ', str(text)) # Remove all the special characters from text\n",
    "\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # Remove all single characters from text\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove any character that isn't alphabetical\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    words = word_tokenize(text) # tokenizes the text into words\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer() #  used to reduce words to their dictionary (base) form\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    Words = [word for word in words if word not in stop_words] #removes stopwords from the list of words.\n",
    "\n",
    "    Words = [word for word in Words if len(word) >= 3] # removes words with a len less than or equal to 3 characters.\n",
    "\n",
    "    indices = np.unique(Words, return_index=True)[1] # gets the indices of unique words in the list of words.\n",
    "    \n",
    "    # to create a cleaned list of words by sorting the indices and retrieving corresponding words from the list:\n",
    "    cleaned_text = np.array(Words)[np.sort(indices)].tolist()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d6a17a-d896-4cc4-b161-27a7b723992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=sentiment.drop('labels',axis=1)\n",
    "y=sentiment.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e6cb0d6-90a9-4a61-bb2f-dd2b7fbf90ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "                                                      text\n",
      "0      I am coming to the borders and I will kill you...\n",
      "1      im getting on borderlands and i will kill you ...\n",
      "2      im coming on borderlands and i will murder you...\n",
      "3      im getting on borderlands 2 and i will murder ...\n",
      "4      im getting into borderlands and i can murder y...\n",
      "...                                                  ...\n",
      "75668  ♥️ Suikoden 2\\n1️⃣ Alex Kidd in Miracle World\\...\n",
      "75669  Thank you to Matching funds Home Depot RW paym...\n",
      "75671  Late night stream with the boys! Come watch so...\n",
      "75675  ⭐️ Toronto is the arts and culture capital of ...\n",
      "75676  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
      "\n",
      "[70251 rows x 1 columns]\n",
      "\n",
      "\n",
      "y:\n",
      "  0          Positive\n",
      "1          Positive\n",
      "2          Positive\n",
      "3          Positive\n",
      "4          Positive\n",
      "            ...    \n",
      "75668       Neutral\n",
      "75669      Positive\n",
      "75671       Neutral\n",
      "75675    Irrelevant\n",
      "75676    Irrelevant\n",
      "Name: labels, Length: 70251, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('x:\\n ',x)\n",
    "print('\\n\\ny:\\n ',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae10f6-0d84-4fed-8da4-202adf8cc388",
   "metadata": {},
   "source": [
    "### Training-Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96a2cae7-7e5f-4b60-b9f7-2b848e0ec1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [process_text(text) for text in list(x['text'])]\n",
    "\n",
    "# Convert text data into numerical features using CountVectorizer: \n",
    "cleaned_text_str = [' '.join(text) for text in cleaned_text]  # joins each list of words into a single string.\n",
    "\n",
    "#initializes a CountVectorizer object,to convert a collection of text documents to a matrix of token counts.\n",
    "vectorizer = CountVectorizer(lowercase=False)  \n",
    "X = vectorizer.fit_transform(cleaned_text_str)\n",
    "\n",
    "# Split the data into training and testing sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier:\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment on the test set:\n",
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a6d32-05ad-4e53-b3e8-15ed6e25d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e7b3b-f890-4e5b-be8b-dd1691614e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9857e838-3d0b-4129-966d-426262ed7496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70251x25302 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 620984 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2066dbe-3118-4ccc-9e6a-46e1bf48257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9835ed9-da10-4c31-8e85-63d676390397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: ['Positive']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already preprocessed the specific text using process_text function\n",
    "processed_specific_text = process_text(\"that is very good\")\n",
    "\n",
    "# Transform the preprocessed text into numerical features\n",
    "specific_text_features = vectorizer.transform(processed_specific_text)  # Pass as a list of tokens\n",
    "\n",
    "# Use the trained Naive Bayes classifier to predict sentiment\n",
    "predicted_sentiment = nb_classifier.predict(specific_text_features)\n",
    "\n",
    "# Output the predicted sentiment\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f24b0a-49f3-4df7-bbf4-895ad70145bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
